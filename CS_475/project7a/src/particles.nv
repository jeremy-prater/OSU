//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Driver 
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_52, texmode_independent
.address_size 32

	// .globl	Particle
.const .align 8 .b8 __internal_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.entry Particle(
	.param .u32 .ptr .global .align 16 Particle_param_0,
	.param .u32 .ptr .global .align 16 Particle_param_1,
	.param .u32 .ptr .global .align 16 Particle_param_2
)
{
	.local .align 8 .b8 	__local_depot0[40];
	.reg .b32 	%SP;
	.reg .b32 	%SPL;
	.reg .pred 	%p<53>;
	.reg .f32 	%f<158>;
	.reg .b32 	%r<252>;
	.reg .f64 	%fd<165>;
	.reg .b64 	%rd<161>;


	mov.u32 	%r251, __local_depot0;
	cvta.local.u32 	%SP, %r251;
	ld.param.u32 	%r97, [Particle_param_0];
	ld.param.u32 	%r98, [Particle_param_1];
	ld.param.u32 	%r96, [Particle_param_2];
	add.u32 	%r99, %SP, 0;
	cvta.to.local.u32 	%r1, %r99;
	cvta.to.local.u32 	%r2, %r99;
	mov.u32 	%r101, %ctaid.x;
	mov.u32 	%r102, %ntid.x;
	mov.b32	%r103, %envreg3;
	mad.lo.s32 	%r104, %r101, %r102, %r103;
	mov.u32 	%r105, %tid.x;
	add.s32 	%r3, %r104, %r105;
	shl.b32 	%r106, %r3, 4;
	add.s32 	%r4, %r97, %r106;
	add.s32 	%r5, %r98, %r106;
	ld.global.v4.f32 	{%f73, %f74, %f75, %f76}, [%r5];
	mov.f32 	%f149, %f76;
	ld.global.v4.f32 	{%f77, %f78, %f79, %f80}, [%r4];
	fma.rn.f32 	%f81, %f74, 0f3F000000, %f78;
	fma.rn.f32 	%f82, %f73, 0f3F000000, %f77;
	fma.rn.f32 	%f83, %f75, 0f3F000000, %f79;
	mov.f32 	%f154, 0f3F800000;
	mov.f32 	%f84, 0fBFC00000;
	mov.f32 	%f85, 0f00000000;
	fma.rn.f32 	%f157, %f85, 0f3F000000, %f75;
	fma.rn.f32 	%f156, %f84, 0f3F000000, %f74;
	fma.rn.f32 	%f155, %f85, 0f3F000000, %f73;
	fma.rn.f32 	%f153, %f85, 0f3E000000, %f83;
	fma.rn.f32 	%f151, %f85, 0f3E000000, %f82;
	fma.rn.f32 	%f152, %f84, 0f3E000000, %f81;
	mul.f32 	%f86, %f152, %f152;
	fma.rn.f32 	%f87, %f151, %f151, %f86;
	fma.rn.f32 	%f88, %f153, %f153, %f87;
	sqrt.approx.f32 	%f89, %f88;
	setp.lt.f32	%p1, %f89, 0f47435000;
	@%p1 bra 	BB0_66;

	cvt.f64.f32	%fd19, %f76;
	add.f64 	%fd20, %fd19, 0d3FF0C152382D7365;
	cvt.rn.f32.f64	%f18, %fd20;
	mov.f32 	%f138, %f77;
	mov.f32 	%f139, %f78;
	mov.f32 	%f140, %f79;
	abs.f32 	%f90, %f77;
	setp.neu.f32	%p2, %f90, 0f00000000;
	@%p2 bra 	BB0_4;

	abs.f32 	%f91, %f78;
	setp.neu.f32	%p3, %f91, 0f00000000;
	@%p3 bra 	BB0_4;

	abs.f32 	%f92, %f79;
	setp.eq.f32	%p4, %f92, 0f00000000;
	@%p4 bra 	BB0_5;

BB0_4:
	mul.f32 	%f93, %f78, %f78;
	fma.rn.f32 	%f94, %f77, %f77, %f93;
	fma.rn.f32 	%f95, %f79, %f79, %f94;
	rsqrt.approx.f32 	%f96, %f95;
	mul.f32 	%f140, %f79, %f96;
	mul.f32 	%f139, %f78, %f96;
	mul.f32 	%f138, %f77, %f96;

BB0_5:
	mul.f32 	%f97, %f74, %f139;
	fma.rn.f32 	%f98, %f73, %f138, %f97;
	fma.rn.f32 	%f99, %f75, %f140, %f98;
	mul.f32 	%f100, %f99, 0f40033333;
	neg.f32 	%f101, %f138;
	neg.f32 	%f102, %f139;
	neg.f32 	%f103, %f140;
	fma.rn.f32 	%f157, %f103, %f100, %f75;
	fma.rn.f32 	%f156, %f102, %f100, %f74;
	fma.rn.f32 	%f155, %f101, %f100, %f73;
	fma.rn.f32 	%f104, %f85, 0f3F000000, %f80;
	fma.rn.f32 	%f105, %f155, 0f3F000000, %f77;
	fma.rn.f32 	%f106, %f156, 0f3F000000, %f78;
	fma.rn.f32 	%f107, %f157, 0f3F000000, %f79;
	fma.rn.f32 	%f153, %f85, 0f3E000000, %f107;
	fma.rn.f32 	%f152, %f84, 0f3E000000, %f106;
	fma.rn.f32 	%f151, %f85, 0f3E000000, %f105;
	add.f32 	%f154, %f104, 0f3E000000;
	abs.f32 	%f109, %f18;
	setp.neu.f32	%p5, %f109, 0f7F800000;
	mov.f32 	%f150, %f18;
	@%p5 bra 	BB0_7;

	mul.rn.f32 	%f38, %f18, %f85;
	mov.f32 	%f150, %f38;

BB0_7:
	mov.f32 	%f39, %f150;
	mul.f32 	%f111, %f39, 0f3F22F983;
	cvt.rni.s32.f32	%r229, %f111;
	cvt.rn.f32.s32	%f112, %r229;
	neg.f32 	%f113, %f112;
	mov.f32 	%f114, 0f3FC90FDA;
	fma.rn.f32 	%f115, %f113, %f114, %f39;
	mov.f32 	%f116, 0f33A22168;
	fma.rn.f32 	%f117, %f113, %f116, %f115;
	mov.f32 	%f118, 0f27C234C5;
	fma.rn.f32 	%f141, %f113, %f118, %f117;
	abs.f32 	%f119, %f39;
	setp.leu.f32	%p6, %f119, 0f47CE4780;
	@%p6 bra 	BB0_17;

	mov.b32 	 %r7, %f39;
	and.b32  	%r8, %r7, -2147483648;
	shl.b32 	%r110, %r7, 8;
	or.b32  	%r9, %r110, -2147483648;
	mov.u32 	%r220, 0;
	mov.u32 	%r221, %r220;
	mov.u32 	%r217, %r220;
	mov.u32 	%r218, %r2;
	mov.pred 	%p7, 0;
	@%p7 bra 	BB0_10;

BB0_9:
	shl.b32 	%r116, %r217, 2;
	mov.u32 	%r117, __cudart_i2opi_f;
	add.s32 	%r118, %r117, %r116;
	ld.const.u32 	%r113, [%r118];
	// inline asm
	{
	mad.lo.cc.u32   %r111, %r113, %r9, %r221;
	madc.hi.u32     %r221, %r113, %r9,  0;
	}
	// inline asm
	st.local.u32 	[%r218], %r111;
	add.s32 	%r217, %r217, 1;
	setp.lt.s32	%p8, %r217, 6;
	add.s32 	%r119, %r116, %r2;
	add.s32 	%r218, %r119, 4;
	mov.u32 	%r220, %r221;
	@%p8 bra 	BB0_9;

BB0_10:
	bfe.u32 	%r120, %r7, 23, 8;
	add.s32 	%r121, %r120, -128;
	shr.u32 	%r122, %r121, 5;
	mov.u32 	%r123, 4;
	sub.s32 	%r124, %r123, %r122;
	st.local.u32 	[%r218], %r220;
	shl.b32 	%r125, %r124, 2;
	add.s32 	%r126, %r125, %r2;
	ld.local.u32 	%r222, [%r126+8];
	ld.local.u32 	%r223, [%r126+4];
	bfe.u32 	%r22, %r7, 23, 5;
	setp.eq.s32	%p9, %r22, 0;
	@%p9 bra 	BB0_12;

	mov.u32 	%r127, 32;
	sub.s32 	%r128, %r127, %r22;
	shr.u32 	%r129, %r223, %r128;
	shl.b32 	%r130, %r222, %r22;
	add.s32 	%r222, %r129, %r130;
	add.s32 	%r216, %r126, 8;
	ld.local.u32 	%r131, [%r216+-8];
	shr.u32 	%r132, %r131, %r128;
	shl.b32 	%r133, %r223, %r22;
	add.s32 	%r223, %r132, %r133;

BB0_12:
	shr.u32 	%r134, %r223, 30;
	shl.b32 	%r135, %r222, 2;
	add.s32 	%r224, %r134, %r135;
	shl.b32 	%r28, %r223, 2;
	shr.u32 	%r136, %r224, 31;
	shr.u32 	%r137, %r222, 30;
	add.s32 	%r29, %r136, %r137;
	setp.eq.s32	%p10, %r136, 0;
	mov.u32 	%r225, %r8;
	mov.u32 	%r226, %r28;
	@%p10 bra 	BB0_14;

	not.b32 	%r138, %r224;
	neg.s32 	%r30, %r28;
	setp.eq.s32	%p11, %r28, 0;
	selp.u32	%r139, 1, 0, %p11;
	add.s32 	%r224, %r139, %r138;
	xor.b32  	%r32, %r8, -2147483648;
	mov.u32 	%r225, %r32;
	mov.u32 	%r226, %r30;

BB0_14:
	mov.u32 	%r34, %r225;
	neg.s32 	%r140, %r29;
	setp.eq.s32	%p12, %r8, 0;
	selp.b32	%r229, %r29, %r140, %p12;
	clz.b32 	%r228, %r224;
	setp.eq.s32	%p13, %r228, 0;
	shl.b32 	%r141, %r224, %r228;
	mov.u32 	%r142, 32;
	sub.s32 	%r143, %r142, %r228;
	shr.u32 	%r144, %r226, %r143;
	add.s32 	%r145, %r144, %r141;
	selp.b32	%r38, %r224, %r145, %p13;
	mov.u32 	%r146, -921707870;
	mul.hi.u32 	%r227, %r38, %r146;
	setp.lt.s32	%p14, %r227, 1;
	@%p14 bra 	BB0_16;

	mul.lo.s32 	%r147, %r38, -921707870;
	shr.u32 	%r148, %r147, 31;
	shl.b32 	%r149, %r227, 1;
	add.s32 	%r227, %r148, %r149;
	add.s32 	%r228, %r228, 1;

BB0_16:
	mov.u32 	%r150, 126;
	sub.s32 	%r151, %r150, %r228;
	shl.b32 	%r152, %r151, 23;
	add.s32 	%r153, %r227, 1;
	shr.u32 	%r154, %r153, 7;
	add.s32 	%r155, %r154, 1;
	shr.u32 	%r156, %r155, 1;
	add.s32 	%r157, %r156, %r152;
	or.b32  	%r158, %r157, %r34;
	mov.b32 	 %f141, %r158;

BB0_17:
	mul.rn.f32 	%f43, %f141, %f141;
	add.s32 	%r45, %r229, 1;
	and.b32  	%r46, %r45, 1;
	setp.eq.s32	%p15, %r46, 0;
	@%p15 bra 	BB0_19;

	mov.f32 	%f120, 0fBAB6061A;
	mov.f32 	%f121, 0f37CCF5CE;
	fma.rn.f32 	%f142, %f121, %f43, %f120;
	bra.uni 	BB0_20;

BB0_19:
	mov.f32 	%f122, 0f3C08839E;
	mov.f32 	%f123, 0fB94CA1F9;
	fma.rn.f32 	%f142, %f123, %f43, %f122;

BB0_20:
	@%p15 bra 	BB0_22;

	mov.f32 	%f124, 0f3D2AAAA5;
	fma.rn.f32 	%f125, %f142, %f43, %f124;
	mov.f32 	%f126, 0fBF000000;
	fma.rn.f32 	%f143, %f125, %f43, %f126;
	bra.uni 	BB0_23;

BB0_22:
	mov.f32 	%f127, 0fBE2AAAA3;
	fma.rn.f32 	%f128, %f142, %f43, %f127;
	fma.rn.f32 	%f143, %f128, %f43, %f85;

BB0_23:
	fma.rn.f32 	%f144, %f143, %f141, %f141;
	@%p15 bra 	BB0_25;

	mov.f32 	%f130, 0f3F800000;
	fma.rn.f32 	%f144, %f143, %f43, %f130;

BB0_25:
	and.b32  	%r159, %r45, 2;
	setp.eq.s32	%p18, %r159, 0;
	@%p18 bra 	BB0_27;

	mov.f32 	%f132, 0fBF800000;
	fma.rn.f32 	%f144, %f144, %f132, %f85;

BB0_27:
	cvt.sat.f32.f32	%f55, %f144;
	cvt.f64.f32	%fd21, %f18;
	add.f64 	%fd1, %fd21, 0d4000C152382D7365;
	abs.f64 	%fd2, %fd1;
	setp.eq.f64	%p19, %fd2, 0d7FF0000000000000;
	mov.f32 	%f133, 0fFFC00000;
	mov.f32 	%f147, %f133;
	@%p19 bra 	BB0_46;

	setp.gt.f64	%p20, %fd2, 0d41E0000000000000;
	@%p20 bra 	BB0_30;
	bra.uni 	BB0_29;

BB0_30:
	mov.b64 	 %rd1, %fd1;
	and.b64  	%rd2, %rd1, -9223372036854775808;
	shr.u64 	%rd3, %rd1, 52;
	bfe.u64 	%rd66, %rd1, 52, 11;
	add.s64 	%rd67, %rd66, 4294966272;
	cvt.u32.u64	%r49, %rd67;
	shr.u32 	%r161, %r49, 6;
	mov.u32 	%r162, 16;
	sub.s32 	%r50, %r162, %r161;
	mov.u32 	%r163, 19;
	sub.s32 	%r164, %r163, %r161;
	mov.u32 	%r165, 18;
	min.s32 	%r51, %r165, %r164;
	setp.gt.s32	%p21, %r50, %r51;
	mov.u64 	%rd141, 0;
	mov.u32 	%r232, %r1;
	@%p21 bra 	BB0_33;

	shl.b64 	%rd69, %rd1, 11;
	or.b64  	%rd4, %rd69, -9223372036854775808;
	add.s32 	%r231, %r50, -1;
	cvt.u32.u64	%r166, %rd3;
	and.b32  	%r167, %r166, 2047;
	add.s32 	%r168, %r167, -1024;
	shr.u32 	%r169, %r168, 6;
	mov.u32 	%r170, 15;
	sub.s32 	%r171, %r170, %r169;
	shl.b32 	%r172, %r171, 3;
	mov.u32 	%r173, __internal_i2opi_d;
	add.s32 	%r230, %r173, %r172;
	mov.u64 	%rd141, 0;
	mov.u32 	%r242, %r1;

BB0_32:
	.pragma "nounroll";
	mov.u32 	%r56, %r242;
	ld.const.u64 	%rd71, [%r230];
	mul.lo.s64 	%rd72, %rd71, %rd4;
	mul.hi.u64 	%rd73, %rd71, %rd4;
	add.s64 	%rd74, %rd141, %rd72;
	setp.lt.u64	%p22, %rd74, %rd141;
	selp.u64	%rd75, 1, 0, %p22;
	add.s64 	%rd141, %rd75, %rd73;
	st.local.u64 	[%r56], %rd74;
	add.s32 	%r230, %r230, 8;
	add.s32 	%r231, %r231, 1;
	setp.lt.s32	%p23, %r231, %r51;
	add.s32 	%r61, %r56, 8;
	mov.u32 	%r232, %r61;
	mov.u32 	%r242, %r61;
	@%p23 bra 	BB0_32;

BB0_33:
	st.local.u64 	[%r232], %rd141;
	ld.local.u64 	%rd142, [%r1+24];
	ld.local.u64 	%rd143, [%r1+16];
	and.b32  	%r174, %r49, 63;
	setp.eq.s32	%p24, %r174, 0;
	@%p24 bra 	BB0_35;

	cvt.u32.u64	%r175, %rd3;
	and.b32  	%r176, %r175, 63;
	shl.b64 	%rd76, %rd142, %r176;
	neg.s64 	%rd77, %rd3;
	cvt.u32.u64	%r177, %rd77;
	and.b32  	%r178, %r177, 63;
	shr.u64 	%rd78, %rd143, %r178;
	or.b64  	%rd142, %rd78, %rd76;
	shl.b64 	%rd79, %rd143, %r176;
	ld.local.u64 	%rd80, [%r1+8];
	shr.u64 	%rd81, %rd80, %r178;
	or.b64  	%rd143, %rd81, %rd79;

BB0_35:
	shr.u64 	%rd82, %rd142, 62;
	cvt.u32.u64	%r179, %rd82;
	shr.u64 	%rd83, %rd143, 62;
	shl.b64 	%rd84, %rd142, 2;
	or.b64  	%rd148, %rd83, %rd84;
	shl.b64 	%rd15, %rd143, 2;
	setp.ne.s64	%p25, %rd15, 0;
	selp.u64	%rd85, 1, 0, %p25;
	or.b64  	%rd86, %rd85, %rd148;
	setp.gt.u64	%p26, %rd86, -9223372036854775808;
	selp.u32	%r180, 1, 0, %p26;
	add.s32 	%r63, %r180, %r179;
	setp.lt.u64	%p27, %rd86, -9223372036854775807;
	mov.u64 	%rd144, %rd2;
	mov.u64 	%rd147, %rd15;
	@%p27 bra 	BB0_37;

	not.b64 	%rd87, %rd148;
	neg.s64 	%rd16, %rd15;
	setp.eq.s64	%p28, %rd15, 0;
	selp.u64	%rd88, 1, 0, %p28;
	add.s64 	%rd148, %rd88, %rd87;
	xor.b64  	%rd18, %rd2, -9223372036854775808;
	mov.u64 	%rd144, %rd18;
	mov.u64 	%rd147, %rd16;

BB0_37:
	mov.u64 	%rd146, %rd147;
	mov.u64 	%rd20, %rd144;
	neg.s32 	%r183, %r63;
	setp.eq.s64	%p29, %rd2, 0;
	selp.b32	%r238, %r63, %r183, %p29;
	mov.u32 	%r236, 0;
	mov.u32 	%r237, %r236;
	setp.lt.s64	%p30, %rd148, 1;
	@%p30 bra 	BB0_39;

BB0_38:
	shr.u64 	%rd89, %rd146, 63;
	shl.b64 	%rd90, %rd148, 1;
	or.b64  	%rd148, %rd89, %rd90;
	shl.b64 	%rd146, %rd146, 1;
	add.s32 	%r237, %r237, -1;
	setp.gt.s64	%p31, %rd148, 0;
	mov.u32 	%r236, %r237;
	@%p31 bra 	BB0_38;

BB0_39:
	mov.u32 	%r235, %r236;
	mul.lo.s64 	%rd150, %rd148, -3958705157555305931;
	mov.u64 	%rd91, -3958705157555305931;
	mul.hi.u64 	%rd149, %rd148, %rd91;
	setp.lt.s64	%p32, %rd149, 1;
	@%p32 bra 	BB0_41;

	shl.b64 	%rd92, %rd149, 1;
	shr.u64 	%rd93, %rd150, 63;
	or.b64  	%rd149, %rd92, %rd93;
	mul.lo.s64 	%rd150, %rd148, -7917410315110611862;
	add.s32 	%r235, %r235, -1;

BB0_41:
	setp.ne.s64	%p33, %rd150, 0;
	selp.u64	%rd94, 1, 0, %p33;
	add.s64 	%rd95, %rd94, %rd149;
	add.s32 	%r184, %r235, 1022;
	cvt.u64.u32	%rd96, %r184;
	shl.b64 	%rd97, %rd96, 52;
	shr.u64 	%rd98, %rd95, 11;
	add.s64 	%rd99, %rd97, %rd98;
	bfe.u64 	%rd100, %rd95, 10, 1;
	add.s64 	%rd101, %rd99, %rd100;
	or.b64  	%rd102, %rd101, %rd20;
	mov.b64 	 %fd161, %rd102;
	bra.uni 	BB0_42;

BB0_29:
	mov.f64 	%fd35, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd22, %fd1, %fd35;
	// inline asm
	cvt.rni.s32.f64 	%r238, %fd22;
	// inline asm
	cvt.rn.f64.s32	%fd36, %r238;
	neg.f64 	%fd32, %fd36;
	mov.f64 	%fd25, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd23, %fd32, %fd25, %fd1;
	// inline asm
	mov.f64 	%fd29, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd27, %fd32, %fd29, %fd23;
	// inline asm
	mov.f64 	%fd33, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd161, %fd32, %fd33, %fd27;
	// inline asm

BB0_42:
	add.s32 	%r71, %r238, 1;
	and.b32  	%r185, %r71, 1;
	setp.eq.b32	%p34, %r185, 1;
	mul.rn.f64 	%fd6, %fd161, %fd161;
	@!%p34 bra 	BB0_44;
	bra.uni 	BB0_43;

BB0_43:
	mov.f64 	%fd38, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd40, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd37, %fd38, %fd6, %fd40;
	// inline asm
	mov.f64 	%fd44, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd41, %fd37, %fd6, %fd44;
	// inline asm
	mov.f64 	%fd48, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd45, %fd41, %fd6, %fd48;
	// inline asm
	mov.f64 	%fd52, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd49, %fd45, %fd6, %fd52;
	// inline asm
	mov.f64 	%fd56, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd53, %fd49, %fd6, %fd56;
	// inline asm
	mov.f64 	%fd60, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd57, %fd53, %fd6, %fd60;
	// inline asm
	mov.f64 	%fd64, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd162, %fd57, %fd6, %fd64;
	// inline asm
	bra.uni 	BB0_45;

BB0_44:
	mov.f64 	%fd66, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd68, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd65, %fd66, %fd6, %fd68;
	// inline asm
	mov.f64 	%fd72, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd69, %fd65, %fd6, %fd72;
	// inline asm
	mov.f64 	%fd76, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd73, %fd69, %fd6, %fd76;
	// inline asm
	mov.f64 	%fd80, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd77, %fd73, %fd6, %fd80;
	// inline asm
	mov.f64 	%fd84, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd81, %fd77, %fd6, %fd84;
	// inline asm
	mul.rn.f64 	%fd86, %fd81, %fd6;
	// inline asm
	fma.rn.f64 	%fd162, %fd86, %fd161, %fd161;
	// inline asm

BB0_45:
	and.b32  	%r186, %r71, 2;
	setp.eq.s32	%p35, %r186, 0;
	neg.f64 	%fd89, %fd162;
	selp.f64	%fd90, %fd162, %fd89, %p35;
	cvt.rn.f32.f64	%f56, %fd90;
	mov.f32 	%f147, %f56;

BB0_46:
	mov.f32 	%f57, %f147;
	cvt.sat.f32.f32	%f59, %f57;
	add.f64 	%fd10, %fd21, 0d4010C152382D7365;
	abs.f64 	%fd11, %fd10;
	setp.eq.f64	%p36, %fd11, 0d7FF0000000000000;
	mov.f32 	%f146, %f133;
	@%p36 bra 	BB0_65;

	setp.gt.f64	%p37, %fd11, 0d41E0000000000000;
	@%p37 bra 	BB0_49;
	bra.uni 	BB0_48;

BB0_49:
	mov.b64 	 %rd33, %fd10;
	and.b64  	%rd34, %rd33, -9223372036854775808;
	shr.u64 	%rd35, %rd33, 52;
	bfe.u64 	%rd104, %rd33, 52, 11;
	add.s64 	%rd105, %rd104, 4294966272;
	cvt.u32.u64	%r73, %rd105;
	shr.u32 	%r188, %r73, 6;
	mov.u32 	%r189, 16;
	sub.s32 	%r74, %r189, %r188;
	mov.u32 	%r190, 19;
	sub.s32 	%r191, %r190, %r188;
	mov.u32 	%r192, 18;
	min.s32 	%r75, %r192, %r191;
	setp.gt.s32	%p38, %r74, %r75;
	mov.u64 	%rd151, 0;
	mov.u32 	%r244, %r1;
	@%p38 bra 	BB0_52;

	shl.b64 	%rd107, %rd33, 11;
	or.b64  	%rd36, %rd107, -9223372036854775808;
	add.s32 	%r243, %r74, -1;
	cvt.u32.u64	%r193, %rd35;
	and.b32  	%r194, %r193, 2047;
	add.s32 	%r195, %r194, -1024;
	shr.u32 	%r196, %r195, 6;
	mov.u32 	%r197, 15;
	sub.s32 	%r198, %r197, %r196;
	shl.b32 	%r199, %r198, 3;
	mov.u32 	%r200, __internal_i2opi_d;
	add.s32 	%r239, %r200, %r199;
	mov.u64 	%rd151, 0;
	mov.u32 	%r241, %r1;

BB0_51:
	.pragma "nounroll";
	ld.const.u64 	%rd109, [%r239];
	mul.lo.s64 	%rd110, %rd109, %rd36;
	mul.hi.u64 	%rd111, %rd109, %rd36;
	add.s64 	%rd112, %rd151, %rd110;
	setp.lt.u64	%p39, %rd112, %rd151;
	selp.u64	%rd113, 1, 0, %p39;
	add.s64 	%rd151, %rd113, %rd111;
	st.local.u64 	[%r241], %rd112;
	add.s32 	%r241, %r241, 8;
	mov.u32 	%r244, %r241;
	add.s32 	%r239, %r239, 8;
	add.s32 	%r243, %r243, 1;
	setp.lt.s32	%p40, %r243, %r75;
	@%p40 bra 	BB0_51;

BB0_52:
	st.local.u64 	[%r244], %rd151;
	ld.local.u64 	%rd152, [%r1+24];
	ld.local.u64 	%rd153, [%r1+16];
	and.b32  	%r201, %r73, 63;
	setp.eq.s32	%p41, %r201, 0;
	@%p41 bra 	BB0_54;

	cvt.u32.u64	%r202, %rd35;
	and.b32  	%r203, %r202, 63;
	shl.b64 	%rd114, %rd152, %r203;
	neg.s64 	%rd115, %rd35;
	cvt.u32.u64	%r204, %rd115;
	and.b32  	%r205, %r204, 63;
	shr.u64 	%rd116, %rd153, %r205;
	or.b64  	%rd152, %rd116, %rd114;
	shl.b64 	%rd117, %rd153, %r203;
	ld.local.u64 	%rd118, [%r1+8];
	shr.u64 	%rd119, %rd118, %r205;
	or.b64  	%rd153, %rd119, %rd117;

BB0_54:
	shr.u64 	%rd120, %rd152, 62;
	cvt.u32.u64	%r206, %rd120;
	shr.u64 	%rd121, %rd153, 62;
	shl.b64 	%rd122, %rd152, 2;
	or.b64  	%rd158, %rd121, %rd122;
	shl.b64 	%rd47, %rd153, 2;
	setp.ne.s64	%p42, %rd47, 0;
	selp.u64	%rd123, 1, 0, %p42;
	or.b64  	%rd124, %rd123, %rd158;
	setp.gt.u64	%p43, %rd124, -9223372036854775808;
	selp.u32	%r207, 1, 0, %p43;
	add.s32 	%r87, %r207, %r206;
	setp.lt.u64	%p44, %rd124, -9223372036854775807;
	mov.u64 	%rd154, %rd34;
	mov.u64 	%rd157, %rd47;
	@%p44 bra 	BB0_56;

	not.b64 	%rd125, %rd158;
	neg.s64 	%rd48, %rd47;
	setp.eq.s64	%p45, %rd47, 0;
	selp.u64	%rd126, 1, 0, %p45;
	add.s64 	%rd158, %rd126, %rd125;
	xor.b64  	%rd50, %rd34, -9223372036854775808;
	mov.u64 	%rd154, %rd50;
	mov.u64 	%rd157, %rd48;

BB0_56:
	mov.u64 	%rd156, %rd157;
	mov.u64 	%rd52, %rd154;
	neg.s32 	%r210, %r87;
	setp.eq.s64	%p46, %rd34, 0;
	selp.b32	%r250, %r87, %r210, %p46;
	mov.u32 	%r248, 0;
	mov.u32 	%r249, %r248;
	setp.lt.s64	%p47, %rd158, 1;
	@%p47 bra 	BB0_58;

BB0_57:
	shr.u64 	%rd127, %rd156, 63;
	shl.b64 	%rd128, %rd158, 1;
	or.b64  	%rd158, %rd127, %rd128;
	shl.b64 	%rd156, %rd156, 1;
	add.s32 	%r249, %r249, -1;
	setp.gt.s64	%p48, %rd158, 0;
	mov.u32 	%r248, %r249;
	@%p48 bra 	BB0_57;

BB0_58:
	mov.u32 	%r247, %r248;
	mul.lo.s64 	%rd160, %rd158, -3958705157555305931;
	mov.u64 	%rd129, -3958705157555305931;
	mul.hi.u64 	%rd159, %rd158, %rd129;
	setp.lt.s64	%p49, %rd159, 1;
	@%p49 bra 	BB0_60;

	shl.b64 	%rd130, %rd159, 1;
	shr.u64 	%rd131, %rd160, 63;
	or.b64  	%rd159, %rd130, %rd131;
	mul.lo.s64 	%rd160, %rd158, -7917410315110611862;
	add.s32 	%r247, %r247, -1;

BB0_60:
	setp.ne.s64	%p50, %rd160, 0;
	selp.u64	%rd132, 1, 0, %p50;
	add.s64 	%rd133, %rd132, %rd159;
	add.s32 	%r211, %r247, 1022;
	cvt.u64.u32	%rd134, %r211;
	shl.b64 	%rd135, %rd134, 52;
	shr.u64 	%rd136, %rd133, 11;
	add.s64 	%rd137, %rd135, %rd136;
	bfe.u64 	%rd138, %rd133, 10, 1;
	add.s64 	%rd139, %rd137, %rd138;
	or.b64  	%rd140, %rd139, %rd52;
	mov.b64 	 %fd163, %rd140;
	bra.uni 	BB0_61;

BB0_48:
	mov.f64 	%fd105, 0d3FE45F306DC9C883;
	mul.rn.f64 	%fd92, %fd10, %fd105;
	// inline asm
	cvt.rni.s32.f64 	%r250, %fd92;
	// inline asm
	cvt.rn.f64.s32	%fd106, %r250;
	neg.f64 	%fd102, %fd106;
	mov.f64 	%fd95, 0d3FF921FB54442D18;
	// inline asm
	fma.rn.f64 	%fd93, %fd102, %fd95, %fd10;
	// inline asm
	mov.f64 	%fd99, 0d3C91A62633145C00;
	// inline asm
	fma.rn.f64 	%fd97, %fd102, %fd99, %fd93;
	// inline asm
	mov.f64 	%fd103, 0d397B839A252049C0;
	// inline asm
	fma.rn.f64 	%fd163, %fd102, %fd103, %fd97;
	// inline asm

BB0_61:
	add.s32 	%r95, %r250, 1;
	and.b32  	%r212, %r95, 1;
	setp.eq.b32	%p51, %r212, 1;
	mul.rn.f64 	%fd15, %fd163, %fd163;
	@!%p51 bra 	BB0_63;
	bra.uni 	BB0_62;

BB0_62:
	mov.f64 	%fd108, 0dBDA8FF8D5A8F03DB;
	mov.f64 	%fd110, 0d3E21EEA7D67FAD92;
	// inline asm
	fma.rn.f64 	%fd107, %fd108, %fd15, %fd110;
	// inline asm
	mov.f64 	%fd114, 0dBE927E4F8E26B8E3;
	// inline asm
	fma.rn.f64 	%fd111, %fd107, %fd15, %fd114;
	// inline asm
	mov.f64 	%fd118, 0d3EFA01A019DDEC33;
	// inline asm
	fma.rn.f64 	%fd115, %fd111, %fd15, %fd118;
	// inline asm
	mov.f64 	%fd122, 0dBF56C16C16C15D69;
	// inline asm
	fma.rn.f64 	%fd119, %fd115, %fd15, %fd122;
	// inline asm
	mov.f64 	%fd126, 0d3FA5555555555551;
	// inline asm
	fma.rn.f64 	%fd123, %fd119, %fd15, %fd126;
	// inline asm
	mov.f64 	%fd130, 0dBFE0000000000000;
	// inline asm
	fma.rn.f64 	%fd127, %fd123, %fd15, %fd130;
	// inline asm
	mov.f64 	%fd134, 0d3FF0000000000000;
	// inline asm
	fma.rn.f64 	%fd164, %fd127, %fd15, %fd134;
	// inline asm
	bra.uni 	BB0_64;

BB0_63:
	mov.f64 	%fd136, 0d3DE5D8FD1FCF0EC1;
	mov.f64 	%fd138, 0dBE5AE5E5A9291691;
	// inline asm
	fma.rn.f64 	%fd135, %fd136, %fd15, %fd138;
	// inline asm
	mov.f64 	%fd142, 0d3EC71DE3567D4896;
	// inline asm
	fma.rn.f64 	%fd139, %fd135, %fd15, %fd142;
	// inline asm
	mov.f64 	%fd146, 0dBF2A01A019BFDF03;
	// inline asm
	fma.rn.f64 	%fd143, %fd139, %fd15, %fd146;
	// inline asm
	mov.f64 	%fd150, 0d3F8111111110F7D0;
	// inline asm
	fma.rn.f64 	%fd147, %fd143, %fd15, %fd150;
	// inline asm
	mov.f64 	%fd154, 0dBFC5555555555548;
	// inline asm
	fma.rn.f64 	%fd151, %fd147, %fd15, %fd154;
	// inline asm
	mul.rn.f64 	%fd156, %fd151, %fd15;
	// inline asm
	fma.rn.f64 	%fd164, %fd156, %fd163, %fd163;
	// inline asm

BB0_64:
	and.b32  	%r213, %r95, 2;
	setp.eq.s32	%p52, %r213, 0;
	neg.f64 	%fd159, %fd164;
	selp.f64	%fd160, %fd164, %fd159, %p52;
	cvt.rn.f32.f64	%f146, %fd160;

BB0_65:
	cvt.sat.f32.f32	%f137, %f146;
	add.s32 	%r215, %r96, %r106;
	st.global.v4.f32 	[%r215], {%f55, %f59, %f137, %f135};
	mov.f32 	%f149, %f18;

BB0_66:
	st.global.v4.f32 	[%r4], {%f151, %f152, %f153, %f154};
	st.global.v4.f32 	[%r5], {%f155, %f156, %f157, %f149};
	ret;
}


  